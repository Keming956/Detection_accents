{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Début du traitement pour la région: England_English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement de output_audio_test - England_English: 100%|██████████| 214/214 [00:00<00:00, 3834.29it/s]\n",
      "Traitement de output_audio_val - England_English: 100%|██████████| 222/222 [00:00<00:00, 5324.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion terminée pour England_English: england_texts.txt\n",
      "\n",
      "Début du traitement pour la région: India_and_South_Asia_(India,_Pakistan,_Sri_Lanka)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement de output_audio_test - India_and_South_Asia_(India,_Pakistan,_Sri_Lanka): 100%|██████████| 346/346 [00:00<00:00, 5580.73it/s]\n",
      "Traitement de output_audio_val - India_and_South_Asia_(India,_Pakistan,_Sri_Lanka): 100%|██████████| 385/385 [00:00<00:00, 6820.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion terminée pour India_and_South_Asia_(India,_Pakistan,_Sri_Lanka): india_texts.txt\n",
      "\n",
      "Début du traitement pour la région: United_States_English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement de output_audio_test - United_States_English: 100%|██████████| 727/727 [00:00<00:00, 7506.55it/s]\n",
      "Traitement de output_audio_val - United_States_English: 100%|██████████| 931/931 [00:00<00:00, 7128.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion terminée pour United_States_English: us_texts.txt\n",
      "\n",
      "Traitement terminé pour toutes les régions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_txt_files_by_region(base_dirs, region_name, output_file):\n",
    "    \"\"\"\n",
    "    # Fusionner les fichiers texte par région\n",
    "    # Paramètres:\n",
    "    # base_dirs: Répertoires de base contenant les données\n",
    "    # region_name: Nom de la région à traiter\n",
    "    # output_file: Fichier de sortie pour stocker les résultats fusionnés\n",
    "    \"\"\"\n",
    "    all_content = []\n",
    "    \n",
    "    for dir_path in base_dirs:\n",
    "        region_path = os.path.join(dir_path, region_name, 'txt')\n",
    "        if os.path.exists(region_path):\n",
    "            # Obtenir la liste des fichiers texte\n",
    "            files = [f for f in os.listdir(region_path) if f.endswith('.txt')]\n",
    "            for file in tqdm(files, desc=f\"Traitement de {os.path.basename(dir_path)} - {region_name}\"):\n",
    "                with open(os.path.join(region_path, file), 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    all_content.append(content)\n",
    "    \n",
    "    # Écrire le contenu fusionné dans le fichier de sortie\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_content))\n",
    "    print(f'Fusion terminée pour {region_name}: {output_file}')\n",
    "\n",
    "# Répertoires de base\n",
    "base_dirs = ['output_audio_test', 'output_audio_val']\n",
    "\n",
    "# Configuration des régions\n",
    "regions = {\n",
    "    'England_English': 'england_texts.txt',\n",
    "    'India_and_South_Asia_(India,_Pakistan,_Sri_Lanka)': 'india_texts.txt',\n",
    "    'United_States_English': 'us_texts.txt'\n",
    "}\n",
    "\n",
    "# Traiter chaque région\n",
    "for region, output_file in regions.items():\n",
    "    print(f\"\\nDébut du traitement pour la région: {region}\")\n",
    "    merge_txt_files_by_region(base_dirs, region, output_file)\n",
    "\n",
    "print(\"\\nTraitement terminé pour toutes les régions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/keming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/keming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Analyse des régions: 100%|██████████| 3/3 [00:00<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyse terminée! Résultats sauvegardés dans word_frequency_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Télécharger les ressources NLTK nécessaires\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def analyze_word_frequency(file_path):\n",
    "    \"\"\"\n",
    "    Analyser la fréquence des mots dans un fichier texte\n",
    "    Retourne les 10 mots les plus fréquents\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    \n",
    "    # Tokenisation du texte\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Suppression des mots vides et de la ponctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Comptage des mots\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    return word_counts.most_common(10)\n",
    "\n",
    "def plot_word_frequency(region_results, output_file):\n",
    "    \"\"\"\n",
    "    Créer un graphique de fréquence des mots\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    words, counts = zip(*region_results)\n",
    "    plt.bar(words, counts)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Top 10 mots les plus fréquents\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_file}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Fichiers à analyser\n",
    "files = {\n",
    "    'Anglais britannique': 'england_texts.txt',\n",
    "    'Anglais indien': 'india_texts.txt',\n",
    "    'Anglais américain': 'us_texts.txt'\n",
    "}\n",
    "\n",
    "# Analyse et sauvegarde des résultats\n",
    "results = {}\n",
    "with open('word_frequency_results.txt', 'w', encoding='utf-8') as result_file:\n",
    "    for region, file_path in tqdm(files.items(), desc=\"Analyse des régions\"):\n",
    "        if os.path.exists(file_path):\n",
    "            top_words = analyze_word_frequency(file_path)\n",
    "            results[region] = top_words\n",
    "            \n",
    "            # Écriture des résultats\n",
    "            result_file.write(f\"\\n{region} - Top 10 mots:\\n\")\n",
    "            for word, count in top_words:\n",
    "                result_file.write(f\"{word}: {count}\\n\")\n",
    "            \n",
    "            # Création du graphique\n",
    "            plot_word_frequency(top_words, f\"word_frequency_{region}\")\n",
    "        else:\n",
    "            print(f\"Fichier non trouvé: {file_path}\")\n",
    "\n",
    "print(\"\\nAnalyse terminée! Résultats sauvegardés dans word_frequency_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1658\n",
      "Total words: 15833\n",
      "Average sentence length: 9.55 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.549457177322076"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_sentence_length(filename):\n",
    "    try:\n",
    "        # Read all sentences from file\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            sentences = f.readlines()\n",
    "        \n",
    "        # Remove empty lines and strip whitespace\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if not sentences:\n",
    "            print(\"Error: File is empty\")\n",
    "            return 0\n",
    "            \n",
    "        # Calculate length of each sentence\n",
    "        word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_sentences = len(sentences)\n",
    "        total_words = sum(word_counts)\n",
    "        avg_length = total_words / total_sentences\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Total sentences: {total_sentences}\")\n",
    "        print(f\"Total words: {total_words}\")\n",
    "        print(f\"Average sentence length: {avg_length:.2f} words\")\n",
    "        \n",
    "        return avg_length\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "analyze_sentence_length(\"us_texts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extra_info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
